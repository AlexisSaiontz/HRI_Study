<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_erbp2lbhnu5a-6.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-6 0}.lst-kix_mn18m3vqwer-1>li{counter-increment:lst-ctn-kix_mn18m3vqwer-1}.lst-kix_mn18m3vqwer-0>li{counter-increment:lst-ctn-kix_mn18m3vqwer-0}.lst-kix_erbp2lbhnu5a-4>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-4}ol.lst-kix_mn18m3vqwer-7.start{counter-reset:lst-ctn-kix_mn18m3vqwer-7 0}ol.lst-kix_mn18m3vqwer-2.start{counter-reset:lst-ctn-kix_mn18m3vqwer-2 0}.lst-kix_erbp2lbhnu5a-3>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-3}ol.lst-kix_erbp2lbhnu5a-0.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-0 0}ol.lst-kix_mn18m3vqwer-4.start{counter-reset:lst-ctn-kix_mn18m3vqwer-4 0}.lst-kix_erbp2lbhnu5a-6>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-6}ol.lst-kix_erbp2lbhnu5a-3.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-3 0}ul.lst-kix_us3mh8vd5gm4-6{list-style-type:none}ul.lst-kix_us3mh8vd5gm4-7{list-style-type:none}ul.lst-kix_us3mh8vd5gm4-8{list-style-type:none}ul.lst-kix_us3mh8vd5gm4-2{list-style-type:none}ul.lst-kix_us3mh8vd5gm4-3{list-style-type:none}ol.lst-kix_mn18m3vqwer-1.start{counter-reset:lst-ctn-kix_mn18m3vqwer-1 0}ul.lst-kix_us3mh8vd5gm4-4{list-style-type:none}ul.lst-kix_us3mh8vd5gm4-5{list-style-type:none}.lst-kix_mn18m3vqwer-4>li{counter-increment:lst-ctn-kix_mn18m3vqwer-4}ol.lst-kix_erbp2lbhnu5a-4.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-4 0}ol.lst-kix_erbp2lbhnu5a-2{list-style-type:none}ol.lst-kix_erbp2lbhnu5a-1{list-style-type:none}ol.lst-kix_mn18m3vqwer-5.start{counter-reset:lst-ctn-kix_mn18m3vqwer-5 0}ol.lst-kix_erbp2lbhnu5a-4{list-style-type:none}ol.lst-kix_erbp2lbhnu5a-3{list-style-type:none}ol.lst-kix_erbp2lbhnu5a-6{list-style-type:none}ol.lst-kix_erbp2lbhnu5a-5{list-style-type:none}.lst-kix_mn18m3vqwer-3>li{counter-increment:lst-ctn-kix_mn18m3vqwer-3}ol.lst-kix_erbp2lbhnu5a-8{list-style-type:none}ol.lst-kix_erbp2lbhnu5a-7{list-style-type:none}ul.lst-kix_us3mh8vd5gm4-0{list-style-type:none}ul.lst-kix_us3mh8vd5gm4-1{list-style-type:none}ol.lst-kix_erbp2lbhnu5a-5.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-5 0}ol.lst-kix_erbp2lbhnu5a-0{list-style-type:none}ol.lst-kix_mn18m3vqwer-0.start{counter-reset:lst-ctn-kix_mn18m3vqwer-0 0}.lst-kix_erbp2lbhnu5a-8>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-8}.lst-kix_erbp2lbhnu5a-5>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-5}ol.lst-kix_mn18m3vqwer-6.start{counter-reset:lst-ctn-kix_mn18m3vqwer-6 0}.lst-kix_erbp2lbhnu5a-2>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-2}.lst-kix_2vgap4lyb873-1>li:before{content:"-  "}.lst-kix_2vgap4lyb873-2>li:before{content:"-  "}.lst-kix_2vgap4lyb873-5>li:before{content:"-  "}.lst-kix_erbp2lbhnu5a-2>li:before{content:"" counter(lst-ctn-kix_erbp2lbhnu5a-2,decimal) ". "}.lst-kix_2vgap4lyb873-3>li:before{content:"-  "}.lst-kix_mn18m3vqwer-6>li{counter-increment:lst-ctn-kix_mn18m3vqwer-6}.lst-kix_erbp2lbhnu5a-3>li:before{content:"" counter(lst-ctn-kix_erbp2lbhnu5a-3,lower-latin) ") "}.lst-kix_erbp2lbhnu5a-4>li:before{content:"(" counter(lst-ctn-kix_erbp2lbhnu5a-4,decimal) ") "}.lst-kix_2vgap4lyb873-4>li:before{content:"-  "}.lst-kix_erbp2lbhnu5a-5>li:before{content:"(" counter(lst-ctn-kix_erbp2lbhnu5a-5,lower-latin) ") "}.lst-kix_erbp2lbhnu5a-6>li:before{content:"(" counter(lst-ctn-kix_erbp2lbhnu5a-6,lower-roman) ") "}ol.lst-kix_erbp2lbhnu5a-2.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-2 0}.lst-kix_mn18m3vqwer-7>li{counter-increment:lst-ctn-kix_mn18m3vqwer-7}.lst-kix_erbp2lbhnu5a-7>li:before{content:"(" counter(lst-ctn-kix_erbp2lbhnu5a-7,lower-latin) ") "}.lst-kix_erbp2lbhnu5a-8>li:before{content:"(" counter(lst-ctn-kix_erbp2lbhnu5a-8,lower-roman) ") "}.lst-kix_2vgap4lyb873-0>li:before{content:"-  "}.lst-kix_erbp2lbhnu5a-0>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-0}.lst-kix_mn18m3vqwer-2>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-2,decimal) ". "}.lst-kix_mn18m3vqwer-1>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-1,lower-roman) ". "}.lst-kix_mn18m3vqwer-3>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-3,lower-latin) ". "}.lst-kix_mn18m3vqwer-0>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-0,lower-latin) ". "}.lst-kix_mn18m3vqwer-4>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-4,lower-roman) ". "}.lst-kix_2vgap4lyb873-6>li:before{content:"-  "}.lst-kix_2vgap4lyb873-7>li:before{content:"-  "}.lst-kix_2vgap4lyb873-8>li:before{content:"-  "}.lst-kix_us3mh8vd5gm4-2>li:before{content:"\0025a0  "}.lst-kix_us3mh8vd5gm4-3>li:before{content:"\0025cf  "}.lst-kix_mn18m3vqwer-8>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-8,decimal) ". "}ol.lst-kix_mn18m3vqwer-8{list-style-type:none}.lst-kix_us3mh8vd5gm4-0>li:before{content:"\0025cf  "}.lst-kix_us3mh8vd5gm4-4>li:before{content:"\0025cb  "}ol.lst-kix_mn18m3vqwer-8.start{counter-reset:lst-ctn-kix_mn18m3vqwer-8 0}.lst-kix_mn18m3vqwer-6>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-6,lower-latin) ". "}.lst-kix_us3mh8vd5gm4-6>li:before{content:"\0025cf  "}.lst-kix_us3mh8vd5gm4-7>li:before{content:"\0025cb  "}.lst-kix_erbp2lbhnu5a-1>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-1}.lst-kix_mn18m3vqwer-5>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-5,decimal) ". "}.lst-kix_mn18m3vqwer-7>li:before{content:"" counter(lst-ctn-kix_mn18m3vqwer-7,lower-roman) ". "}ol.lst-kix_erbp2lbhnu5a-8.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-8 0}.lst-kix_us3mh8vd5gm4-5>li:before{content:"\0025a0  "}ul.lst-kix_2vgap4lyb873-8{list-style-type:none}.lst-kix_us3mh8vd5gm4-8>li:before{content:"\0025a0  "}ul.lst-kix_2vgap4lyb873-2{list-style-type:none}ol.lst-kix_mn18m3vqwer-4{list-style-type:none}ul.lst-kix_2vgap4lyb873-3{list-style-type:none}.lst-kix_erbp2lbhnu5a-7>li{counter-increment:lst-ctn-kix_erbp2lbhnu5a-7}ol.lst-kix_mn18m3vqwer-5{list-style-type:none}ul.lst-kix_2vgap4lyb873-0{list-style-type:none}ol.lst-kix_mn18m3vqwer-6{list-style-type:none}ul.lst-kix_2vgap4lyb873-1{list-style-type:none}ol.lst-kix_mn18m3vqwer-7{list-style-type:none}ul.lst-kix_2vgap4lyb873-6{list-style-type:none}ol.lst-kix_mn18m3vqwer-0{list-style-type:none}ul.lst-kix_2vgap4lyb873-7{list-style-type:none}ol.lst-kix_mn18m3vqwer-1{list-style-type:none}ul.lst-kix_2vgap4lyb873-4{list-style-type:none}ol.lst-kix_mn18m3vqwer-2{list-style-type:none}ul.lst-kix_2vgap4lyb873-5{list-style-type:none}ol.lst-kix_mn18m3vqwer-3{list-style-type:none}.lst-kix_mn18m3vqwer-8>li{counter-increment:lst-ctn-kix_mn18m3vqwer-8}.lst-kix_mn18m3vqwer-5>li{counter-increment:lst-ctn-kix_mn18m3vqwer-5}ol.lst-kix_erbp2lbhnu5a-7.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-7 0}ol.lst-kix_erbp2lbhnu5a-1.start{counter-reset:lst-ctn-kix_erbp2lbhnu5a-1 0}.lst-kix_erbp2lbhnu5a-1>li:before{content:"" counter(lst-ctn-kix_erbp2lbhnu5a-1,upper-latin) ". "}ol.lst-kix_mn18m3vqwer-3.start{counter-reset:lst-ctn-kix_mn18m3vqwer-3 0}.lst-kix_us3mh8vd5gm4-1>li:before{content:"\0025cb  "}.lst-kix_erbp2lbhnu5a-0>li:before{content:"" counter(lst-ctn-kix_erbp2lbhnu5a-0,upper-roman) ". "}.lst-kix_mn18m3vqwer-2>li{counter-increment:lst-ctn-kix_mn18m3vqwer-2}ol{margin:0;padding:0}table td,table th{padding:0}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:135pt;border-top-color:#000000;border-bottom-style:solid}.c26{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:135pt;border-top-color:#000000;border-bottom-style:solid}.c27{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:135pt;border-top-color:#000000;border-bottom-style:solid}.c19{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:135pt;border-top-color:#000000;border-bottom-style:solid}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center;height:11pt}.c0{padding-top:0pt;text-indent:36pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c10{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Times New Roman";font-style:italic}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:center}.c22{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c5{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c21{border-spacing:0;border-collapse:collapse;margin-right:auto}.c13{color:#000000;text-decoration:none;vertical-align:baseline;font-style:italic}.c16{color:#000000;text-decoration:underline;vertical-align:baseline;font-style:normal}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c24{font-size:9pt;font-family:"Times New Roman";font-weight:700}.c12{font-weight:400;font-size:9pt;font-family:"Times New Roman"}.c18{font-weight:700;font-size:11pt;font-family:"Times New Roman"}.c15{font-weight:400;font-size:10pt;font-family:"Arial"}.c29{font-weight:400;font-size:12pt;font-family:"Times New Roman"}.c2{font-size:10pt;font-family:"Times New Roman";font-weight:400}.c31{font-size:19pt;font-family:"Times New Roman";font-weight:700}.c30{font-weight:400;font-size:11pt;font-family:"Arial"}.c11{font-weight:700;font-size:10pt;font-family:"Times New Roman"}.c28{max-width:540pt;padding:36pt 36pt 36pt 36pt}.c3{background-color:#ffffff;font-style:italic}.c17{font-style:italic}.c25{text-decoration:underline}.c23{height:11pt}.c14{height:0pt}.c9{background-color:#ffffff}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c9 c28"><div><p class="c22 c23"><span class="c5 c30"></span></p></div><p class="c7"><span class="c5 c31">Testing the Effect of Vocal Intonation and Facial Expression</span></p><p class="c7"><span class="c31">on Human Perception of Robot Intelligence</span></p><p class="c7"><span class="c13 c29">Stephanie Hickman, Abhishek Nayar, Alexis Saiontz</span></p><p class="c7 c23"><span class="c13 c29"></span></p><p class="c1"><span class="c11">Abstract</span></p><p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This paper explores the effects of vocal intonation and facial expression on human perception of robot intelligence. We have built a novel system to isolate the interaction modalities of voice based interaction and visual feedback. We have run an experiment where participants were asked to interact with a uni-modal version of our system and we quantitatively and qualitatively analyzed the reported measures of robot intelligence. This paper presents our study, our motivations, and the underlying technologies. We end with a discussion of our findings. In our limited tests we saw statistically significant results indicating that facial expressiveness had a larger impact on perception of robot intelligence.</span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c1"><span class="c11">I. </span><span class="c11">Introduction</span></p><p class="c1"><span class="c11">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5 c2">As technology moves forward at breakneck speeds, robots (both physical and digital) are becoming increasingly ubiquitous in everyday life. From bots on platforms such as Facebook Messenger or Slack, to voice-based interfaces like Amazon&rsquo;s Alexa, and even experimental and upstart ventures such as Jeebo, we see robots involved in a wide variety of use-cases and mediums of interaction. In the long run, if robots are to truly become saturated in human life they must learn to interact with us in the most natural way possible. </span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Current mainstream modalities of interaction include three of the five basic senses - sight, sound, and touch. Smell and taste are still experimental areas and as such are not considered in this paper. We will focus on the modalities of sight and sound, and in particular how these features affect how subjects perceive a robot. </span></p><p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There is a lack of literature in modality comparison. Many studies exist exploring the effects of single modalities, but multi-modal studies appear to be rare. This paper explores the effects of vocal intonation and facial expression on human perception of robot intelligence. In particular we are trying to make an educated prediction as to which modality contributes most to greater impressions of intelligence. We have built a novel test system, Jem, to assist in this study. We endowed Jem with the ability to switch between one of four states: </span><span class="c2 c17">expressive voice + expressive face (EV/EF)</span><span class="c2">, </span><span class="c2 c17">expressive voice + non-expressive face(EV/NEF)</span><span class="c2">, </span><span class="c2 c17">non-expressive voice + expressive face(NEV/EF)</span><span class="c2">, </span><span class="c2 c17">non-expressive voice + non-expressive face(NEV/NEF)</span><span class="c5 c2">. This set-up lends itself well to a two-by-two or four-by-four matrix style research studies. </span></p><p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This paper presents our work testing the hypothesis that an emotive face would lead to higher perceptions of animacy and intelligence than an intoned voice. We tested this by exposing test subjects to various configurations of Jem in a two-by-two style study. We tested Jem&rsquo;s </span><span class="c2 c17">EV/NEF </span><span class="c2">and </span><span class="c2 c17">NEV/EF </span><span class="c2">modes, which will be referred to as EV and EF going forward. </span><span class="c2">We collected quantitative and qualitative data after each interaction in the form of a survey</span><span class="c2">&nbsp;and </span><span class="c2">questionnaire, and used this data to inform the predictions and results discussed later on in this paper</span><span class="c5 c2">.</span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c1"><span class="c5 c11">II. Motivations</span></p><p class="c1"><span class="c11">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5 c2">As previously mentioned, there is a lack of comparative research on methods of human-robot interaction. This study aims to both address this issue as well as to create a novel test system that can be used and expanded in the future. Our system, Jem, is capable of Wizard of Oz control but can easily be adapted to be autonomous or perform in specialized applications for future research.</span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This research is important for the fields of robot design, human-computer interaction, and robotics. Human-robot interaction is a new area of research and the impact of various design decisions on outcomes in this field has yet to be understood.</span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c1"><span class="c5 c11">III. Research</span></p><p class="c1"><span class="c11">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c5 c2">Though there is a lack of research into the comparative effectiveness of modalities of interaction, especially in the perception of intelligence domain, we do find studies of uni-modal interaction elements in past literature. Our paper focuses on robot interaction via facial expression and vocal intonation.</span></p><p class="c0"><span class="c2">Facial expression and other paralinguistic cues have been shown to alone carry a large amount of information, and sometimes the entire message itself </span><span class="c2">(Breazeal, 2002)</span><span class="c2">. According to </span><span class="c2">DiSalvo et al. (2002)</span><span class="c5 c2">&nbsp;the presence of facial features on a robot is extremely important. 62% of the perception of &ldquo;humanness&rdquo; is contributed by facial features. Of the face, the eyes, nose and mouth have been shown to have the greatest impact on the perception of &ldquo;humanness&rdquo; (DiSavo et al., 2002). </span></p><p class="c0"><span class="c5 c2">DiSalvo et al., while espousing the virtues of facial features, also cautions against building a face that falls into the &ldquo;uncanny valley&rdquo;- an area where a robot face (or other feature) is close to a human&rsquo;s but falls just short. This leads to individuals reporting higher levels of discomfort during interactions than with a less humanlike robot. </span></p><p class="c0"><span class="c2">While designing our robot system we took inspiration from DiSalvo et al. as well as </span><span class="c2">Dehn and Mulkin (1999). </span><span class="c5 c2">Dehn and Mulkin discuss the increasingly important role of animated robotic agents in academia as well as commercial applications. Advocates of animated agents hold that animated features create a more human-like, engaging, and motivating computer system (Dehn &amp; Mulken, 1999). Given this and the features highlighted by DiSalvo et al. we chose to build an embodied robot with an animated face.</span></p><p class="c0"><span class="c2">As for vocal intonation, there are several examples of agents that combine natural language with an animated or embodied avatar (</span><span class="c2">Breazeal, 2002</span><span class="c2">). Rea from MIT Media Labs &nbsp;(</span><span class="c2">Cassell et al., 2000</span><span class="c2">), Steve developed at USC (</span><span class="c2">Rickeland Johnson, 2000</span><span class="c2">), and Greta by Rosis et al., to name a few. Brazeal, in the </span><span class="c2 c3">International Journal of Human-Computer Studies</span><span class="c2">, examines the various effects emotion has on speech. Emotion affects human-synthesized speech by tuning variables such as pitch, tone, etc. as identified by </span><span class="c2">Janet Cahn in her Master&rsquo;s thesis &ldquo;</span><span class="c2">Generating Expression in Synthesized Speech&rdquo; (Brazeal, 2002). Research shows that there exist few systems developed to synthesize emotive speech. At the forefront of emotive voice synthesis is </span><span class="c2">Google&rsquo;s WaveNet,</span><span class="c5 c2">&nbsp;which employs neural networks and deep learning to synthesize emotive vocal intonations. Given that we lacked funds and access to cutting edge technologies (such as those of Google) we were limited to freely available synthesized voices that came prepackaged with the HTML5 Speech API.</span></p><p class="c0 c23"><span class="c5 c2"></span></p><p class="c1"><span class="c5 c11">IV. Experiment</span></p><p class="c0"><span class="c13 c11">A. Software</span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are three main parts of the software underlying Jem- the face, the database, and the controller. The face and controller are static websites connected to a persistent database. All websites were hosted on Github pages, and Firebase (A Google BAAS platform) was used for our database. All interactive code was written in Javascript.</span></p><p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c2 c25">The Face:</span><span class="c5 c2">&nbsp;At a base level, Jem&rsquo;s face is a static webpage composed of HTML and CSS. The code was built in a manner that the addition of certain CSS classes would alter the appearance of the web-page using CSS3 animations. In this way, by dynamically adding or removing various classes, we are able to make Jem smile, blink, scowl, etc. as shown in Figure 1. </span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The face is also connected to a persistent database (discussed in the next section). The face web page reads in the values stored in this database and dynamically adds or removes CSS classes using Javascript and jQuery. </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 332.14px; height: 112.50px;"><img alt="" src="images/image6.png" style="width: 332.14px; height: 112.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c10">Figure 1: The faces of Jem</span></p><p class="c4"><span class="c5 c2"></span></p><p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c2 c25">The Database:</span><span class="c5 c2">&nbsp;For ease of development and for real-time control, Firebase was chosen to serve as our applications backend. Firebase allows for the updating and serving of data in real-time across devices. </span></p><p class="c0"><span class="c5 c2">This allowed us to build a system that we could rely on to change robot state instantaneously, a critical component of a Wizard of Oz experiment.</span></p><p class="c0"><span class="c2">In our database we stored several variables such as </span><span class="c2 c17">isSmiling</span><span class="c2">, </span><span class="c2 c17">wordsToSpeak</span><span class="c5 c2">, etc. Each of these corresponded to an individual robot action. These variables were updated and changed by the robot controller interface.</span></p><p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c2 c25">The Controller:</span><span class="c2">&nbsp;The controller, like the face web page, is also a static website connected to our Firebase backend. From this controller one is able to update all digital features of the robot such as its emotional state, its current configuration (</span><span class="c2 c17">emotive voice + non-emotive face </span><span class="c2">or </span><span class="c2 c17">non-emotive voice + emotive face</span><span class="c5 c2">), and all other facial/vocal features of the robot. &nbsp;</span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The controller also had the ability to reset the robot back to a null state and displayed the robot&rsquo;s last spoken utterances to the human controller (also for Wizard of Oz purposes). Figure 2 shows the robot control interface. </span></p><p class="c1"><span class="c2">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 330.70px; height: 166.50px;"><img alt="" src="images/image12.png" style="width: 330.70px; height: 166.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c10">Figure 2: Robot Control Interface</span></p><p class="c4"><span class="c5 c2"></span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Because of the way the websites were designed (as static pages connected to a separate persistent data store), we were able to build a system that could concurrently run across numerous devices and one that could be controlled from both a desktop, laptop, tablet, mobile, or any other internet connected device. </span></p><p class="c0"><span class="c13 c11">B. Hardware</span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The hardware of the robot, Jem, is comprised of three parts: a mobile phone, a 3D-printed &ldquo;body&rdquo;, and a foam cover that holds the phone in place. &nbsp;</span></p><p class="c0"><span class="c2 c25">The Body:</span><span class="c5 c2">&nbsp;This portion of the robot was originally designed using Solidworks 3D-modeling software. Due to its size, the robot had to be split into four parts to fit the parameters of the 3D-printers available to us. The pieces were then assembled using duct tape, hot glue, and rubber cement. The goal of the design was to create a robot that has a form different from what our participants may typically interact with, as to reduce bias as much as possible. Considering this, the design is purposefully not humanoid, as we suspected that a human-like body could subconsciously impact the participants&rsquo; expectations of intelligence. As shown in Figure 3, the form took on a geometric form of an oval-like cylinder with bumps scattered across the top. The reason for the oval-like cylinder shape is to keep the robot as simple as possible, as we hypothesized that a complex shape could imply a higher level of intelligence. The reason for the bumps is to create some sort of randomization that could make the form unique to other objects that the participants may interact with in their daily life (as a uniformly square shape may resemble a television). </span></p><p class="c0"><span class="c2 c5">Also shown in Figure 3, Jem was painted green, a gender-neutral color. It has been shown that the gender of a social robot can influence its &ldquo;persuasive power&rdquo; and perceived &ldquo;task suitability&rdquo; (Tay et al., 2014). Fear of gender bias also affected the name we gave our robot- Jem. Jem was meant to be seen as both a variation of &ldquo;Jim&rdquo; (a name typically used for males) and &ldquo;Gem&rdquo;, a more female sounding name.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 342.50px; height: 256.05px;"><img alt="" src="images/image3.jpg" style="width: 342.50px; height: 256.05px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c10">Figure 3: Jem fully assembled with non-expressive face.</span></p><p class="c4"><span class="c10"></span></p><p class="c0"><span class="c2 c25">The Phone: </span><span class="c5 c2">The phone was meant to serve as both the face and brain of our robot. We used a Motorola Moto X (2nd Generation) with an android operating system that would be compatible with the application we developed. Our software application was run on the phone and controllable via another phone, or computer connected to the Internet.</span></p><p class="c0"><span class="c2 c25">The Cover:</span><span class="c5 c2">&nbsp;The purpose of the foam cover was to keep the phone in place as well as cover parts of the screen that were not necessary to our experiment (e.g. time and battery life shown at the top of the screen). The cover attaches to the body with duct tape, which makes it easily removable while also securely stabilizing the phone during the experiment.</span></p><p class="c0"><span class="c13 c11">C. Experiment Design</span></p><p class="c0"><span class="c5 c2">32 participants were recruited. All participants were Yale University students aged 18 to 25. Participants were informed that they would be working with a robot named Jem. We used the general wording of the following script: </span></p><p class="c0"><span class="c13 c2">&ldquo;We taught Jem to recognize shapes and colors. We have taught Jem to solve different types of puzzles using its knowledge of shapes and colors. &nbsp;We are also teaching Jem to process speech from different voices, and to learn to follow directions when solving puzzles. For this experiment, you will be looking through a slideshow of 12 puzzles with Jem. These are all types of puzzles that Jem has been taught to solve, but the exact content is novel. You will ask Jem a question from your script. Jem will then try to process your question, and answer correctly based on the image. After Jem answers, please give Jem feedback about whether the answer was correct or incorrect. Be sure to answer with affirmative or negative language. This feedback will allow Jem to learn.&rdquo;</span></p><p class="c0"><span class="c5 c2">A second experimenter escorted participants to the study room. They were told to keep the slides matched up with the questions they were asking and to enunciate clearly. They were told to use the keyword &ldquo;JEM&rdquo; to start each question. They were also told not to touch Jem because Jem was positioned to view the visual stimuli. Finally, they were told to mark if Jem answered questions correctly or incorrectly on their sheet.</span></p><p class="c0"><span class="c5 c2">They were positioned with Jem on one side and the computer screen on the other, as seen in Figure 4. Participants were given a sheet containing the list of questions to ask Jem, which also contained the correct answers. They were also told to control a numbered slideshow, the slide number corresponding to each question. After each question was asked and answered, the participants gave Jem feedback. </span></p><p class="c0 c23"><span class="c5 c2"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 318.50px; height: 253.25px;"><img alt="" src="images/image9.png" style="width: 318.50px; height: 253.25px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c10">Figure 4: Experiment setup photo taken just right of participant chair.</span></p><p class="c4"><span class="c10"></span></p><p class="c0"><span class="c5 c2">The slides/questions demanded that Jem be able to identify and count colored shapes, identify identical (or not) figures that were different in size and orientation, and count the number of irregular lines. Jem always answered the same way, and answered a total of 4 questions incorrectly. For one of these answers, question 12, Jem also seemed to misunderstand the question.</span></p><p class="c0"><span class="c5 c2">Jem would respond to feedback with the words &ldquo;I&rsquo;m glad I got that right!&rdquo; or &ldquo;I&rsquo;m sad I got that wrong.&rdquo; </span></p><p class="c0"><span class="c2">The participants believed that Jem was acting autonomously. In reality, an experimenter was on a screen-sharing Skype call and the experimenter could hear the participant and see what slide was displayed on the screen. Jem&rsquo;s responses were inputted remotely as outlined in section </span><span class="c2 c17">IV.A</span><span class="c5 c2">. </span></p><p class="c0"><span class="c5 c2">Two variations of Jem were used in this experiment, Expressive Face variant (EF) and Expressive Voice variant (EV). EF used the monotone voice, but would present facial expressions&ndash; a smile when saying &ldquo;I&rsquo;m glad I got that right!&rdquo; and a frown when saying &ldquo;I&rsquo;m sad I got that wrong.&rdquo; EV did not present these facial expressions but used a more expressive, human-like voice throughout the experiment.</span></p><p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c11 c17">D. Data Collection</span></p><p class="c0"><span class="c5 c2">After the experiment, the participants filled out a three part survey to collect qualitative and quantitative data. </span></p><p class="c0"><span class="c2 c25">Part One </span><span class="c2 c17 c25">(adapted from Bartneck et al, 2009)</span><span class="c2 c16">:</span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The participant is asked about their general perception of the quiz administered and its difficulty. </span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Next, the participant is asked to rank Jem by different adjectives pairs. These adjectives fall into four categories: intelligence, likeability, anthropomorphism, and animacy. Participants ranked each of these traits on a scale from 1 to 5; &nbsp;One adjective pair falling under intelligence is incompetent and competent; participants rank Jem on a scale of 1 to 5 where 1 represents incompetent and 5 represents competent. &nbsp;</span></p><p class="c0"><span class="c16 c2">Part Two:</span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The participant is asked what they noticed about Jem&rsquo;s reaction to feedback, its voice, and any facial expressions. They are also asked to describe their perception of Jem&rsquo;s overall intelligence in qualitative terms.</span></p><p class="c0"><span class="c16 c2">Part Three:</span></p><p class="c1"><span class="c5 c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The participant is asked further questions about Jem&rsquo;s ability to understand them. They are also asked about their past exposure to Computer Science and Artificial Intelligence.</span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c1"><span class="c5 c11">V. Results</span></p><p class="c0"><span class="c13 c11">A. Exclusions</span></p><p class="c0"><span class="c2">There were a total of 32 participants, and of these 32 we found only 28 usable data points. We excluded four of the data points</span><span class="c2">&nbsp;due to a technical error made by the operator of Jem. The robot got an extra question wrong during two of the trials, which could possibly skew their perception of Jem&rsquo;s intelligence. One participant whose data we excluded only filled out a portion of the survey, leaving us with incomplete data. The last data point excluded was due to an error with the quiz: a slide was missing and therefore when the robot answered correctly it was obvious to the participant that Jem had been pre-programmed, skewing their perception of Jem&rsquo;s intelligence.</span></p><p class="c0"><span class="c5 c2">A potential issue we faced was that participants may not have noticed Jem&rsquo;s facial expressions in the EF variant. Our survey asked participants to describe any facial expressions they noticed. The plan was to exclude participants in the EF variant who did not notice the face; there was only one participant who failed to notice Jems facial expression but they were excluded anyway due to error(s) mentioned above.</span></p><p class="c0"><span class="c11 c17">B. Data and Analysis</span></p><p class="c0"><span class="c5 c2">We performed ANOVAs for the numerical data in the two groups, EF and EV.</span></p><p class="c0"><span class="c2">Our data was for 28 participants total&ndash; 14 in each variant. We first analyzed the qualitative data to record an </span><span class="c2 c17">AI_exposure </span><span class="c2">variable for each participant. Participants who had exposure (as defined by a semester or more of A.I. related coursework or professional experience) were given &nbsp;</span><span class="c2 c17">AI_exposure </span><span class="c2">= 1; for the rest of the participants </span><span class="c2 c17">AI_exposure </span><span class="c5 c2">= 0. We used this variable as a cofactor in our analysis. </span></p><p class="c0"><span class="c2">We used a general linear model to calculate between-subject effects for the EF and EV groups. We both analyzed averages for the broad categories outlined above (likability, intelligence, animacy, and anthropomorphism) and compared/tested the reported values for each sub-category/adjective pair. With this we were able to find values for </span><span class="c2 c17">overall_likability</span><span class="c2">,</span><span class="c2 c17">&nbsp;overall_intelligence</span><span class="c2">, </span><span class="c2 c17">overall_animacy, </span><span class="c2">and </span><span class="c2 c17">overall_anthropomorphism</span><span class="c5 c2">. </span></p><p class="c0"><span class="c2">Between-subject effects in overall likability, overall animacy, overall anthropomorphism, and each sub-trait in these categories was found to be insignificant (see Figures 5-8). One of our findings did imply that the EF variant seemed more natural to participants. Analysis of the sub-trait natural/unnatural was not significant but did imply a trend. &nbsp;The EV mean was 2.5 with a standard deviation of .76, while the EF mean was 2.0 with a standard deviation of .961 (see Figure 11). With a </span><span class="c2 c17">p</span><span class="c5 c2">&nbsp;value of 0.12, the implication seems to be that the expressive face was found to be less natural than the expressive voice.</span></p><p class="c0 c23"><span class="c5 c2"></span></p><p class="c0 c23"><span class="c5 c2"></span></p><p class="c0 c23"><span class="c5 c2"></span></p><a id="t.24c743fa2290cf4a6ca2651d8da458b5f1da379b"></a><a id="t.0"></a><table class="c21"><tbody><tr class="c14"><td class="c19" colspan="1" rowspan="1"><p class="c8 c23"><span class="c5 c2"></span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">EF Mean</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">EV Mean</span></p></td><td class="c19" colspan="1" rowspan="1"><p class="c8"><span class="c13 c2">p</span></p></td></tr><tr class="c14"><td class="c27" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">Intelligence</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c8"><span class="c5 c11">4.29</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">3.57</span></p></td><td class="c27" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">0.02</span></p></td></tr><tr class="c14"><td class="c26" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">Overall intelligence</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c8"><span class="c5 c11">4.02</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">3.594</span></p></td><td class="c26" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">0.056</span></p></td></tr><tr class="c14"><td class="c20" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">Sensibility</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c8"><span class="c5 c11">4.0</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">3.36</span></p></td><td class="c20" colspan="1" rowspan="1"><p class="c8"><span class="c5 c2">0.08</span></p></td></tr></tbody></table><p class="c6"><span class="c17 c24">Table 1: &nbsp;Significant questionnaire data. n=28. Intelligence and Sensibility were measured on a scale from 1 to 5. Overall intelligence was measured as the average of intelligence, sensibility, competence, ignorance, and responsibility.</span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c0"><span class="c2">Table 1 summarizes the statistically significant data we found; details are also found in Figures 9-11. The overall intelligence of Jem was found to be significantly higher in the EF variant than the EV variant (</span><span class="c2 c17">p &nbsp;= 0.056). </span><span class="c2">Upon closer examination, this was due to two sub-traits: Intelligence (where 1 = unintelligent and 5 = intelligent) and Sensibility (where 1 = foolish and 5 = sensible). The EF variant was found to be significantly more sensible than the EV variant (</span><span class="c2 c17">p = 0.08). </span><span class="c2">Most strikingly, the EF variant was rated significantly higher on the intelligence sub-trait than the EV variant (</span><span class="c2 c17">p = 0.02). </span><span class="c5 c2">The average difference between the scores for this sub-trait was .72. </span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 177.33px;"><img alt="" src="images/image11.png" style="width: 288.00px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><p class="c6"><span class="c10">Figure 5: Averages shown for overall likeability with 95% confidence intervals. P-value of 0.911. EV mean = 3.979, </span></p><p class="c6"><span class="c10">EF mean = 4.007.</span></p><p class="c4"><span class="c10"></span></p><p class="c4"><span class="c10"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 177.33px;"><img alt="" src="images/image4.png" style="width: 288.00px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><p class="c6"><span class="c10">Figure 6: Averages shown for overall animacy with 95% confidence intervals. P-value of 0.829. EV mean = 2.998, </span></p><p class="c6"><span class="c10">EF mean = 2.942.</span></p><p class="c4"><span class="c10"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 177.33px;"><img alt="" src="images/image10.png" style="width: 288.00px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><p class="c6"><span class="c2">&nbsp;</span><span class="c10">Figure 7: Averages shown for overall anthropomorphism with 95% confidence intervals. P-value of 0.848. &nbsp;EV mean = 2.395, </span></p><p class="c6"><span class="c10">EF mean = 2.447.</span></p><p class="c4"><span class="c10"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 177.33px;"><img alt="" src="images/image5.png" style="width: 288.00px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><p class="c6"><span class="c10">&nbsp;Figure 8: Averages shown for overall intelligence with 95% confidence intervals. P-value of 0.056. EV mean = 3.594, </span></p><p class="c6"><span class="c10">EF mean = 4.020.</span></p><p class="c4"><span class="c10"></span></p><p class="c4"><span class="c10"></span></p><p class="c4"><span class="c10"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 178.67px;"><img alt="" src="images/image8.png" style="width: 288.00px; height: 178.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><p class="c6"><span class="c2">&nbsp;</span><span class="c10">Figure 9: Averages shown for sensibility with 95% confidence intervals. P-value of 0.080. &nbsp;EV mean = 3.360, </span></p><p class="c6"><span class="c10">EF mean = 3.997.</span></p><p class="c4"><span class="c10"></span></p><p class="c4"><span class="c10"></span></p><p class="c4"><span class="c10"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 177.33px;"><img alt="" src="images/image2.png" style="width: 288.00px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><p class="c6"><span class="c2">&nbsp;</span><span class="c10">Figure 10: Averages shown for intelligence with 95% confidence intervals. P-value of 0.020.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 177.33px;"><img alt="" src="images/image1.png" style="width: 288.00px; height: 177.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Chart"></span></p><p class="c6"><span class="c10">&nbsp;Figure 11: Averages shown for natural with 95% confidence intervals. P-value of 0.160.</span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c0"><span class="c13 c11">C. Notable Observations</span></p><p class="c0"><span class="c2">The last question of the survey was setup to simulate the robot misunderstanding the question. There was an entanglement of lines and Jem was asked where the line ended, but responded &ldquo;there are 9 lines.&rdquo;, which technically is a true statement (see Figure 12). </span><span class="c2">Many</span><span class="c5 c2">&nbsp;of the participants, in both groups, would either ask Jem the question again, giving the robot another chance to get it correct. </span></p><p class="c0"><span class="c5 c2">Some would try to explain to the experimenter as they left that it was the participant&rsquo;s error, not Jem, who caused Jem to fail the last question. Some of the participants insisted that Jem did in fact give the correct answer and refused to mark a point of the quiz. </span></p><p class="c0"><span class="c5 c2">On their questionnaire, one participant explicitly stated that they were &ldquo;glad Jem got most correct&rdquo; because they were &ldquo;&lsquo;rooting&rsquo; for Jem&rdquo;. Overall, participants were very attached to Jem and were willing to implicate themselves to support Jem&rsquo;s intelligence. Moving forward this could be an interesting phenomena to isolate and study.</span></p><p class="c22"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 288.00px; height: 217.33px;"><img alt="" src="images/image7.png" style="width: 288.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c22"><span class="c5 c12">Figure 12: Question 12 from the experiment quiz - Which number does the line end on?</span></p><p class="c22 c23"><span class="c5 c12"></span></p><p class="c0"><span class="c11 c17">D. </span><span class="c11 c17">Discussion</span></p><p class="c1"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Overall, our results confirmed our hypothesis, that an emotive face would have a stronger effect on human perception of robot intelligence than a more human-like/intoned voice. </span><span class="c2">We conclude that within our study, with our particular facial expressions and voices, facial expressiveness had a statistically significant advantage over vocal intonation in a robot&rsquo;s communicated intelligence level. We cannot make conclusive claims about the universal scalability of our results, but they may imply that in certain scenarios, &nbsp;facial expressions are a stronger communicator of robot intelligence than vocal intonation. Though we studied modalities separately, we predict that the best results and greatest perception of intelligence will be achieved with a multi-modal system.</span></p><p class="c1 c23"><span class="c5 c2"></span></p><p class="c22"><span class="c11">VI. </span><span class="c5 c11">References</span></p><p class="c22"><span class="c2">Bartneck, Christoph, et al. &ldquo;Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots.&rdquo; </span><span class="c2 c17">International journal of social robotics </span><span class="c2">1.1 (2009): 71-81.</span></p><p class="c22 c23"><span class="c5 c2"></span></p><p class="c22"><span class="c2 c9">Breazeal, Cynthia. &quot;Emotion and sociable humanoid robots.&quot; </span><span class="c2 c3">International Journal of Human-Computer Studies</span><span class="c5 c2 c9">&nbsp;59.1 (2003): 119-155.</span></p><p class="c22 c23"><span class="c5 c2 c9"></span></p><p class="c22"><span class="c5 c2 c9">Breazeal, Cynthia. &quot;Emotive qualities in robot speech.&quot; Intelligent Robots and Systems, 2001. Proceedings. 2001 IEEE/RSJ International Conference on. Vol. 3. IEEE, 2001.</span></p><p class="c22 c23"><span class="c5 c2 c9"></span></p><p class="c22"><span class="c2 c9">Cahn, Janet E. </span><span class="c2 c3">Generating expression in synthesized speech</span><span class="c5 c2 c9">. MS thesis. Massachusetts Institute of Technology, Dept. of Architecture, 1989.</span></p><p class="c22 c23"><span class="c5 c2 c9"></span></p><p class="c22"><span class="c2 c9">Cassell, Justine. </span><span class="c2 c3">Embodied conversational agents</span><span class="c5 c2 c9">. MIT press, 2000.</span></p><p class="c22 c23"><span class="c5 c2 c9"></span></p><p class="c22"><span class="c2 c9">Dehn, Doris M., and Susanne Van Mulken. &quot;The impact of animated interface agents: a review of empirical research.&quot; </span><span class="c2 c3">International journal of human-computer studies</span><span class="c5 c2 c9">&nbsp;52.1 (2000): 1-22.</span></p><p class="c22 c23"><span class="c5 c2 c9"></span></p><p class="c22"><span class="c2 c9">Dieleman, Sander, et al. &quot;Wavenet: A generative model for raw audio.&quot; (2016).</span></p><p class="c22 c23"><span class="c5 c2 c9"></span></p><p class="c22"><span class="c2 c9">DiSalvo, Carl F., et al. &quot;All robots are not created equal: the design and perception of humanoid robot heads.&quot; </span><span class="c2 c3">Proceedings of the 4th conference on Designing interactive systems: processes, practices, methods, and techniques</span><span class="c5 c2 c9">. ACM, 2002.</span></p><p class="c22 c23"><span class="c5 c2 c9"></span></p><p class="c22"><span class="c2 c9">Johnson, W. Lewis, Jeff W. Rickel, and James C. Lester. &quot;Animated pedagogical agents: Face-to-face interaction in interactive learning environments.&quot; </span><span class="c2 c3">International Journal of Artificial intelligence in education</span><span class="c2 c9">&nbsp;11.1 (2000): 47-78.</span></p><p class="c22 c23"><span class="c5 c2 c9"></span></p><p class="c22"><span class="c2">Tay, Benedict, et al. &quot;When Stereotypes Meet Robots: The Double-edge Sword of Robot Gender and Personality in Human-robot Interaction.&quot;</span><span class="c2 c17">Computers in Human Behavior</span><span class="c2">&nbsp;38 (2014): 75-84.</span></p></body></html>
